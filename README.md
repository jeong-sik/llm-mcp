# llm-mcp ğŸ«

[![Version](https://img.shields.io/badge/version-0.2.0-blue.svg)](https://github.com/jeong-sik/llm-mcp)
[![OCaml](https://img.shields.io/badge/OCaml-5.x-orange.svg)](https://ocaml.org/)
[![MCP](https://img.shields.io/badge/MCP-2025--11--25-blue.svg)](https://spec.modelcontextprotocol.io/)
[![Status](https://img.shields.io/badge/status-Production%20Ready-green.svg)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Multi-LLM MCP Server written in OCaml (native binary).

> **Note**: Python ë²„ì „ì€ ë³„ë„ ì•„ì¹´ì´ë¸Œë¡œ ì´ë™ë¨ (ì´ ì €ì¥ì†Œì—ëŠ” OCaml ë²„ì „ë§Œ ìœ ì§€)

**MAGI Trinity (ì‚¼ë‘ì •ì¹˜)**: Unified MCP wrapper for multiple LLM CLIs:

| # | ë©¤ë²„ | ì—­í•  | ë„êµ¬ | ëª¨ë¸ |
|---|------|------|------|------|
| 1 | ğŸ”¬ **MELCHIOR** | ê³¼í•™ì | `codex` | GPT-5.2 |
| 2 | ğŸª **BALTHASAR** | ê±°ìš¸ | `claude-cli` | Opus 4.5 |
| 3 | ğŸ¯ **CASPER** | ì „ëµê°€ | `gemini` | Gemini 3 Pro |

+ `ollama` - ë²”ìš© ë¡œì»¬ LLM (devstral, deepseek-r1, qwen3 ë“±)
+ ğŸ’¡ **í˜ë¥´ì†Œë‚˜**: `system_prompt` íŒŒë¼ë¯¸í„°ë¡œ ì»¤ìŠ¤í…€ ì—­í•  ì„¤ì • ê°€ëŠ¥

## Why OCaml?

| Feature | Python | OCaml |
|---------|--------|-------|
| Type Safety | Runtime errors | **Compile-time verification** âœ¨ |
| Performance | Interpreted | **Native binary** |
| Deployment | pip, venv, uvicorn | **Single binary** |
| Pattern Matching | `match` statement | **Exhaustive** |
| Code Size | ~480 lines | ~400 lines |

## Quick Start

> Requires OCaml >= 5.4.0 (recommended: `opam switch create . 5.4.0`)

```bash
# Install dependencies
opam install . --deps-only

# Build
dune build

# Run (HTTP mode, default)
dune exec llm-mcp

# Run (HTTP mode, custom port)
dune exec llm-mcp -- --port 8932

# Run (stdio mode, legacy)
dune exec llm-mcp -- --stdio

# Install globally
dune install
```

## Usage

### HTTP mode (default)

```bash
llm-mcp --port 8932

# Health check
curl http://localhost:8932/health
```

### stdio mode (legacy)

```bash
echo '{"jsonrpc":"2.0","method":"tools/list","id":1}' | llm-mcp --stdio | jq
```

### HTTP mode (example call)

```bash
# Start server
llm-mcp --port 8932

# Health check
curl http://localhost:8932/health

# Call tool
curl -X POST http://localhost:8932/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "id": 1,
    "method": "tools/call",
    "params": {
      "name": "gemini",
      "arguments": {
        "prompt": "Hello, world!",
        "model": "gemini-3-pro-preview"
      }
    }
  }'
```

## MCP Configuration

Add to `~/.mcp.json` (stdio):

```json
{
  "mcpServers": {
    "llm-mcp": {
      "command": "llm-mcp",
      "args": ["--stdio"]
    }
  }
}
```

Or for HTTP mode (recommended):

```json
{
  "mcpServers": {
    "llm-mcp": {
      "type": "http",
      "url": "http://127.0.0.1:8932/mcp"
    }
  }
}
```

## Tools

### Token-saving mode

- Use `"budget_mode": true` to apply token-saving defaults.
- Or set `LLM_MCP_BUDGET_MODE=1` to enable budget defaults when parameters are omitted.

Budget defaults:
- Gemini: `thinking_level = "low"`
- Claude: `ultrathink = false`
- Codex: `reasoning_effort = "medium"`
- **Response format**: `compact` (when budget_mode=true)

### Compact Protocol v1.3 ğŸš€

LLM-to-LLM í†µì‹  ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ì¤‘ í¬ë§· ì‘ë‹µ ì‹œìŠ¤í…œ. MAGI ë©€í‹°-ì—ì´ì „íŠ¸ í˜‘ì—… ì‹œ í† í° ë¹„ìš©ì„ **ìµœëŒ€ 70%** ì ˆê°í•©ë‹ˆë‹¤.

> ğŸ“– **ìƒì„¸ ìŠ¤í™**: [`docs/PROTOCOL.md`](docs/PROTOCOL.md)

#### Response Formats

| Format | Prefix | Overhead | Best For |
|--------|--------|----------|----------|
| Verbose | `{` | 0% (baseline) | ë””ë²„ê¹…, ì‚¬ëŒì´ ì½ì„ ë•Œ |
| Compact DSL | `RES\|` | **-40~50%** âœ¨ | ì§§ì€ ì‘ë‹µ (<50 bytes) |
| Binary | `M` | +33% | Base64 í˜¸í™˜ì„± ìš°ì„  |
| Base85 | `A` | +25% | ì¤‘ê°„ ì‘ë‹µ (50-500 bytes) |
| Compressed | `Z` | **-50~70%** âœ¨ | ê¸´ ì‘ë‹µ (>500 bytes) |
| Auto | - | varies | ìë™ ìµœì  ì„ íƒ |

#### Usage

```bash
# 1. Parameter (per-request)
curl -X POST http://localhost:8932/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "method": "tools/call",
    "params": {
      "name": "codex",
      "arguments": {
        "prompt": "Hello",
        "response_format": "compact"
      }
    }
  }'

# 2. Environment variable (server default)
LLM_MCP_BUDGET_MODE=true llm-mcp --port 8932
```

#### Response Examples

**Verbose (JSON)**:
```json
{"model":"codex","returncode":0,"response":"Hello"}
```

**Compact DSL** (`RES|`):
```
RES|OK|X5|0|Hello
```

**Base85** (`A`):
```
A{base85_encoded_msgpack}
```

**Compressed** (`Z`):
```
Z{zlib_compressed_base85}
```

#### Streaming Delta Protocol

ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìš© ë¸íƒ€ ì—…ë°ì´íŠ¸:

```
D|F|Hello           # Full: ì „ì²´ êµì²´
D|+|, world!        # Append: ëì— ì¶”ê°€
D|R|7|World         # Replace: ìœ„ì¹˜ 7ë¶€í„° êµì²´
```

#### Client Libraries

| Language | Location | Tests | Example |
|----------|----------|-------|---------|
| TypeScript | `clients/typescript/` | 18 | [`example-usage.ts`](clients/typescript/example-usage.ts) |
| Python | `clients/python/` | 20 | [`example_usage.py`](clients/python/example_usage.py) |

```typescript
// TypeScript
import { decode, decodeBase85 } from './compact-decoder';
const response = decode(serverOutput);
// Run example: npx tsx example-usage.ts
```

```python
# Python
from compact_decoder import decode, decode_base85
response = decode(server_output)
# Run example: python example_usage.py
```

#### When to Use

| Scenario | Recommended Format |
|----------|-------------------|
| Human debugging | `verbose` |
| MAGI consensus voting | `compact` âœ¨ |
| Large code responses | `compressed` âœ¨ |
| Default (budget_mode=true) | `auto` |

### gemini
Run Gemini CLI (CASPER in MAGI)

> **Note**: Gemini CLIëŠ” `thinking_level`ì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ ([Issue #6693](https://github.com/google-gemini/gemini-cli/issues/6693) ì°¸ì¡°).
> `thinking_level: "high"` ì„¤ì • ì‹œ prompt engineering workaround ì ìš©:
> *"Think step by step carefully, considering multiple perspectives and edge cases before answering."*

```json
{
  "prompt": "What is 2+2?",
  "model": "gemini-3-pro-preview",
  "thinking_level": "high",
  "budget_mode": false,
  "yolo": false,
  "timeout": 300
}
```

### claude-cli
Run Claude Code CLI (BALTHASAR in MAGI)

```json
{
  "prompt": "Explain this code",
  "model": "opus",
  "ultrathink": true,
  "budget_mode": false,
  "system_prompt": null,
  "output_format": "text",
  "allowed_tools": [],
  "working_directory": "/tmp",
  "timeout": 300
}
```

### codex
Run OpenAI Codex CLI (MELCHIOR in MAGI)

```json
{
  "prompt": "Write a function",
  "model": "gpt-5.2",
  "reasoning_effort": "xhigh",
  "budget_mode": false,
  "sandbox": "workspace-write",
  "working_directory": null,
  "timeout": 300
}
```

#### Codex CLI Direct Usage (Non-MCP)

MCP ì—†ì´ ì§ì ‘ Codex CLIë¥¼ í˜¸ì¶œí•  ë•Œì˜ ì˜¬ë°”ë¥¸ ë¬¸ë²•:

```bash
# âœ… CORRECT: codex exec ì‚¬ìš© (non-interactive)
echo 'Review this code...' | codex exec -c 'model="gpt-5.2"' -

# âœ… CORRECT: í”„ë¡¬í”„íŠ¸ë¥¼ ì¸ìë¡œ ì§ì ‘ ì „ë‹¬
codex exec -c 'model="gpt-5.2"' "Explain this function"

# âŒ WRONG: -pëŠ” profile ì˜µì…˜ (prompt ì•„ë‹˜!)
codex -p "prompt"  # Error: config profile not found

# âŒ WRONG: --json ì˜µì…˜ ì—†ìŒ
codex --json ...   # Error: unexpected argument

# âŒ WRONG: -a auto ì—†ìŒ
codex -a auto ...  # Error: invalid value (possible: untrusted, on-failure, on-request, never)
```

**ì£¼ìš” ì˜µì…˜**:
| ì˜µì…˜ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| `-c 'model="..."'` | ëª¨ë¸ ì„ íƒ | `-c 'model="gpt-5.2"'` |
| `-a never` | ìŠ¹ì¸ ì •ì±… | ìë™ ì‹¤í–‰ |
| `exec` | Non-interactive ëª¨ë“œ | `codex exec ...` |
| `-` (stdin) | íŒŒì´í”„ ì…ë ¥ | `echo "..." \| codex exec -` |

**Code Review ì „ìš©**:
```bash
# Built-in review subcommand
codex exec review

# ë˜ëŠ” ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
echo 'Review for security issues...' | codex exec -c 'model="gpt-5.2"' -
```

### ollama
Run local LLM via Ollama (completely free, no API key)

```json
{
  "prompt": "Explain this code",
  "model": "devstral",
  "system_prompt": null,
  "temperature": 0.7,
  "timeout": 300
}
```

#### Ollama Model Tiers

> âš ï¸ **MCP Timeout**: Claude Code has **60 second hard limit**. Choose models accordingly!

| Tier | Cold Start | VRAM | Models | MCP Compatible |
|------|------------|------|--------|----------------|
| **Tier 1 - Fast** âš¡ | <10s | <8GB | `qwen3:1.7b`, `llama3.2`, `exaone3.5` | âœ… Recommended |
| **Tier 2 - Medium** | 10-30s | 8-20GB | `devstral`, `mistral-small-24b` | âœ… Usually OK |
| **Tier 3 - Heavy** ğŸ¢ | >60s | >40GB | `atom-80b`, `glm4-32k` (84GB) | âŒ Pre-warm required |

**Pre-warm heavy models** (background, before MCP call):
```bash
# Warm up glm4-32k in background
curl http://localhost:11434/api/generate -d '{"model": "glm4-32k:latest", "prompt": "hi", "stream": false}' &

# Check loaded models
curl http://localhost:11434/api/ps | jq '.models[].name'
```

## Development

```bash
# Format code
dune fmt

# Run tests
dune test

# Build in watch mode
dune build -w
```

## License

MIT
