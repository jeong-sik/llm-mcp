# llm-mcp ğŸ«

Multi-LLM MCP Server written in OCaml (native binary).

> **Note**: Python ë²„ì „ì€ `features/_archived/llm-mcp-python/`ì— ì•„ì¹´ì´ë¸Œë¨

**MAGI Pentarchy (ì˜¤ë‘ì •ì¹˜)**: Unified MCP wrapper for multiple LLM CLIs:

| # | ë©¤ë²„ | ì—­í•  | ë„êµ¬ | ëª¨ë¸ |
|---|------|------|------|------|
| 1 | ğŸ”¬ **MELCHIOR** | ê³¼í•™ì | `codex` | GPT-5.2 |
| 2 | ğŸª **BALTHASAR** | ê±°ìš¸ | `claude-cli` | Opus 4.5 |
| 3 | ğŸ¯ **CASPER** | ì „ëµê°€ | `gemini` | Gemini 3 Pro |
| 4 | ğŸ§® **ADAM** | FP ì² í•™ì | `adam` | Mistral-Nemo (local) |
| 5 | âš”ï¸ **SEELE** | ê·¹ë‹¨ ë¦¬ë·°ì–´ | `seele` | Devstral (local) |

+ `ollama` - ë²”ìš© ë¡œì»¬ LLM (devstral, deepseek-r1, qwen3 ë“±)

## Why OCaml?

| Feature | Python | OCaml |
|---------|--------|-------|
| Type Safety | Runtime errors | **Compile-time verification** âœ¨ |
| Performance | Interpreted | **Native binary** |
| Deployment | pip, venv, uvicorn | **Single binary** |
| Pattern Matching | `match` statement | **Exhaustive** |
| Code Size | ~480 lines | ~400 lines |

## Quick Start

> Requires OCaml >= 5.4.0 (recommended: `opam switch create . 5.4.0`)

```bash
# Install dependencies
opam install . --deps-only

# Build
dune build

# Run (HTTP mode, default)
dune exec llm-mcp

# Run (HTTP mode, custom port)
dune exec llm-mcp -- --port 8932

# Run (stdio mode, legacy)
dune exec llm-mcp -- --stdio

# Install globally
dune install
```

## Usage

### HTTP mode (default)

```bash
llm-mcp --port 8932

# Health check
curl http://localhost:8932/health
```

### stdio mode (legacy)

```bash
echo '{"jsonrpc":"2.0","method":"tools/list","id":1}' | llm-mcp --stdio | jq
```

### HTTP mode (example call)

```bash
# Start server
llm-mcp --port 8932

# Health check
curl http://localhost:8932/health

# Call tool
curl -X POST http://localhost:8932/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "id": 1,
    "method": "tools/call",
    "params": {
      "name": "gemini",
      "arguments": {
        "prompt": "Hello, world!",
        "model": "gemini-3-pro-preview"
      }
    }
  }'
```

## MCP Configuration

Add to `~/.mcp.json` (stdio):

```json
{
  "mcpServers": {
    "llm-mcp": {
      "command": "llm-mcp",
      "args": ["--stdio"]
    }
  }
}
```

Or for HTTP mode (recommended):

```json
{
  "mcpServers": {
    "llm-mcp": {
      "type": "http",
      "url": "http://127.0.0.1:8932/mcp"
    }
  }
}
```

## Tools

### Token-saving mode

- Use `"budget_mode": true` to apply token-saving defaults.
- Or set `LLM_MCP_BUDGET_MODE=1` to enable budget defaults when parameters are omitted.

Budget defaults:
- Gemini: `thinking_level = "low"`
- Claude: `ultrathink = false`
- Codex: `reasoning_effort = "medium"`
- **Response format**: `compact` (when budget_mode=true)

### Compact Protocol v1.3 ğŸš€

LLM-to-LLM í†µì‹  ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ì¤‘ í¬ë§· ì‘ë‹µ ì‹œìŠ¤í…œ. MAGI ë©€í‹°-ì—ì´ì „íŠ¸ í˜‘ì—… ì‹œ í† í° ë¹„ìš©ì„ **ìµœëŒ€ 70%** ì ˆê°í•©ë‹ˆë‹¤.

> ğŸ“– **ìƒì„¸ ìŠ¤í™**: [`docs/PROTOCOL.md`](docs/PROTOCOL.md)

#### Response Formats

| Format | Prefix | Overhead | Best For |
|--------|--------|----------|----------|
| Verbose | `{` | 0% (baseline) | ë””ë²„ê¹…, ì‚¬ëŒì´ ì½ì„ ë•Œ |
| Compact DSL | `RES\|` | **-40~50%** âœ¨ | ì§§ì€ ì‘ë‹µ (<50 bytes) |
| Binary | `M` | +33% | Base64 í˜¸í™˜ì„± ìš°ì„  |
| Base85 | `A` | +25% | ì¤‘ê°„ ì‘ë‹µ (50-500 bytes) |
| Compressed | `Z` | **-50~70%** âœ¨ | ê¸´ ì‘ë‹µ (>500 bytes) |
| Auto | - | varies | ìë™ ìµœì  ì„ íƒ |

#### Usage

```bash
# 1. Parameter (per-request)
curl -X POST http://localhost:8932/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "method": "tools/call",
    "params": {
      "name": "codex",
      "arguments": {
        "prompt": "Hello",
        "response_format": "compact"
      }
    }
  }'

# 2. Environment variable (server default)
LLM_MCP_BUDGET_MODE=true llm-mcp --port 8932
```

#### Response Examples

**Verbose (JSON)**:
```json
{"model":"codex","returncode":0,"response":"Hello"}
```

**Compact DSL** (`RES|`):
```
RES|OK|X5|0|Hello
```

**Base85** (`A`):
```
A{base85_encoded_msgpack}
```

**Compressed** (`Z`):
```
Z{zlib_compressed_base85}
```

#### Streaming Delta Protocol

ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìš© ë¸íƒ€ ì—…ë°ì´íŠ¸:

```
D|F|Hello           # Full: ì „ì²´ êµì²´
D|+|, world!        # Append: ëì— ì¶”ê°€
D|R|7|World         # Replace: ìœ„ì¹˜ 7ë¶€í„° êµì²´
```

#### Client Libraries

| Language | Location | Tests | Example |
|----------|----------|-------|---------|
| TypeScript | `clients/typescript/` | 18 | [`example-usage.ts`](clients/typescript/example-usage.ts) |
| Python | `clients/python/` | 20 | [`example_usage.py`](clients/python/example_usage.py) |

```typescript
// TypeScript
import { decode, decodeBase85 } from './compact-decoder';
const response = decode(serverOutput);
// Run example: npx tsx example-usage.ts
```

```python
# Python
from compact_decoder import decode, decode_base85
response = decode(server_output)
# Run example: python example_usage.py
```

#### When to Use

| Scenario | Recommended Format |
|----------|-------------------|
| Human debugging | `verbose` |
| MAGI consensus voting | `compact` âœ¨ |
| Large code responses | `compressed` âœ¨ |
| Default (budget_mode=true) | `auto` |

### gemini
Run Gemini CLI (CASPER in MAGI)

> **Note**: Gemini CLIëŠ” `thinking_level`ì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ ([Issue #6693](https://github.com/google-gemini/gemini-cli/issues/6693) ì°¸ì¡°).
> `thinking_level: "high"` ì„¤ì • ì‹œ prompt engineering workaround ì ìš©:
> *"Think step by step carefully, considering multiple perspectives and edge cases before answering."*

```json
{
  "prompt": "What is 2+2?",
  "model": "gemini-3-pro-preview",
  "thinking_level": "high",
  "budget_mode": false,
  "yolo": false,
  "timeout": 300
}
```

### claude-cli
Run Claude Code CLI (BALTHASAR in MAGI)

```json
{
  "prompt": "Explain this code",
  "model": "opus",
  "ultrathink": true,
  "budget_mode": false,
  "system_prompt": null,
  "output_format": "text",
  "allowed_tools": [],
  "working_directory": "/tmp",
  "timeout": 300
}
```

### codex
Run OpenAI Codex CLI (MELCHIOR in MAGI)

```json
{
  "prompt": "Write a function",
  "model": "gpt-5.2",
  "reasoning_effort": "xhigh",
  "budget_mode": false,
  "sandbox": "workspace-write",
  "working_directory": null,
  "timeout": 300
}
```

#### Codex CLI Direct Usage (Non-MCP)

MCP ì—†ì´ ì§ì ‘ Codex CLIë¥¼ í˜¸ì¶œí•  ë•Œì˜ ì˜¬ë°”ë¥¸ ë¬¸ë²•:

```bash
# âœ… CORRECT: codex exec ì‚¬ìš© (non-interactive)
echo 'Review this code...' | codex exec -c 'model="gpt-5.2"' -

# âœ… CORRECT: í”„ë¡¬í”„íŠ¸ë¥¼ ì¸ìë¡œ ì§ì ‘ ì „ë‹¬
codex exec -c 'model="gpt-5.2"' "Explain this function"

# âŒ WRONG: -pëŠ” profile ì˜µì…˜ (prompt ì•„ë‹˜!)
codex -p "prompt"  # Error: config profile not found

# âŒ WRONG: --json ì˜µì…˜ ì—†ìŒ
codex --json ...   # Error: unexpected argument

# âŒ WRONG: -a auto ì—†ìŒ
codex -a auto ...  # Error: invalid value (possible: untrusted, on-failure, on-request, never)
```

**ì£¼ìš” ì˜µì…˜**:
| ì˜µì…˜ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| `-c 'model="..."'` | ëª¨ë¸ ì„ íƒ | `-c 'model="gpt-5.2"'` |
| `-a never` | ìŠ¹ì¸ ì •ì±… | ìë™ ì‹¤í–‰ |
| `exec` | Non-interactive ëª¨ë“œ | `codex exec ...` |
| `-` (stdin) | íŒŒì´í”„ ì…ë ¥ | `echo "..." \| codex exec -` |

**Code Review ì „ìš©**:
```bash
# Built-in review subcommand
codex exec review

# ë˜ëŠ” ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
echo 'Review for security issues...' | codex exec -c 'model="gpt-5.2"' -
```

### ollama
Run local LLM via Ollama (completely free, no API key)

```json
{
  "prompt": "Explain this code",
  "model": "devstral",
  "system_prompt": null,
  "temperature": 0.7,
  "timeout": 300
}
```

#### Ollama Model Tiers

> âš ï¸ **MCP Timeout**: Claude Code has **60 second hard limit**. Choose models accordingly!

| Tier | Cold Start | VRAM | Models | MCP Compatible |
|------|------------|------|--------|----------------|
| **Tier 1 - Fast** âš¡ | <10s | <8GB | `qwen3:1.7b`, `llama3.2`, `exaone3.5` | âœ… Recommended |
| **Tier 2 - Medium** | 10-30s | 8-20GB | `devstral`, `mistral-small-24b` | âœ… Usually OK |
| **Tier 3 - Heavy** ğŸ¢ | >60s | >40GB | `atom-80b`, `glm4-32k` (84GB) | âŒ Pre-warm required |

**Pre-warm heavy models** (background, before MCP call):
```bash
# Warm up glm4-32k in background
curl http://localhost:11434/api/generate -d '{"model": "glm4-32k:latest", "prompt": "hi", "stream": false}' &

# Check loaded models
curl http://localhost:11434/api/ps | jq '.models[].name'
```

### adam
ADAM - Functional Programming Philosopher (Mistral-Nemo local)

> âœ… **Tier 1-2 Model**: 7.1GB VRAM, MCP compatible (~10s cold start)

4th member of MAGI Pentarchy. Pure functional advocate, algebraic effects expert.

```json
{
  "prompt": "How should I design Effect Handlers in OCaml 5?",
  "timeout": 300
}
```

Use cases: OCaml 5.x effects, type system design, GADT discussions.

### seele
SEELE - Extreme Production Code Reviewer (Devstral local)

> â„¹ï¸ **Tier 2 Model**: Uses `devstral` (~14GB VRAM). Usually OK if already loaded.

5th member of MAGI Pentarchy. Harsh code quality gatekeeper.

```json
{
  "prompt": "Review this code for production readiness",
  "code_context": "function fetchData() { return fetch('/api').then(r => r.json()) }",
  "timeout": 300
}
```

Zero tolerance for: `any` types, missing error handling, no tests.

### local_agent
**Local Agent** - Autonomous coding agent using Ollama with function calling ğŸ¤–

> ğŸš€ **NEW**: Run local LLMs as autonomous agents with tool access!

Runs a local Ollama model in an agentic loop. The agent can:
- **bash**: Execute shell commands
- **read**: Read file contents
- **write**: Write files
- **web_search**: Search the web (via Gemini)
- **code_generate**: Generate code (via Codex)
- **analyze**: Deep analysis (via Claude)
- **masc_broadcast**: Multi-agent communication
- **mcp_call**: Call external MCP servers (HTTP & stdio)

#### External MCP Support

`local_agent` can call tools on other MCP servers configured in `~/.mcp.json` or `~/me/.mcp.json`:

**HTTP MCP servers** (context7, MASC, etc.):
```json
{
  "mcpServers": {
    "context7": {
      "type": "http",
      "url": "https://mcp.context7.com/mcp"
    }
  }
}
```

**stdio MCP servers** (maestro, playwright, etc.):
```json
{
  "mcpServers": {
    "maestro": {
      "command": "maestro",
      "args": ["mcp"]
    }
  }
}
```

**Usage in prompts**:
```
Call mcp_call with server="context7", tool="resolve-library-id", arguments={"libraryName": "react"}
```

Or use the `mcp__server__tool` pattern:
```
Call mcp__maestro__list_devices with arguments={}
```

```json
{
  "prompt": "List all Python files and count their lines",
  "model": "llama3.2:latest",
  "max_turns": 5,
  "timeout": 60,
  "working_directory": "/path/to/project"
}
```

#### Function Calling ì§€ì› ëª¨ë¸

> ğŸ† **ì¶”ì²œ**: `mistral-nemo` ë˜ëŠ” `qwen2.5:7b` (ê°€ì¥ ì•ˆì •ì )

**âœ… FC ì§€ì› í™•ì¸ (21ê°œ ëª¨ë¸)** - 2026-01-13 ë°°ì¹˜ í…ŒìŠ¤íŠ¸:

| Family | Models | Size Range | Notes |
|--------|--------|------------|-------|
| **Qwen 2.5** | `qwen2.5:1.5b`, `qwen2.5:7b`, `qwen2.5-coder:7b/14b/32b` | 1.5B-32B | ğŸ† ê°€ì¥ ì•ˆì •ì  |
| **Qwen 3** | `qwen3:0.6b`, `qwen3:1.7b`, `qwen3-coder:30b` | 0.6B-30B | FC ì•ˆì • |
| **Mistral** | `mistral-nemo:latest/12b`, `ministral-3:3b/14b` | 3B-14B | ğŸ† ë¹ ë¥´ê³  ì•ˆì •ì  |
| **Devstral** | `devstral:latest`, `devstral-small-2:24b` | 14B-24B | ì½”ë”© íŠ¹í™” |
| **Others** | `llama3.2`, `phi4-mini`, `smollm2:1.7b`, `Falcon-H1R-7B` | 1.7B-7B | ê°œë³„ í™•ì¸ |
| **Abliterated** | `huihui_ai/qwen2.5-coder-abliterate:32b`, `huihui_ai/qwen3-abliterated:32b` | 32B | FC ì§€ì› |
| **Fine-tuned** | `goekdenizguelmez/JOSIEFIED-Qwen3:8b` | 8B | FC ì§€ì› |

**âŒ FC ë¯¸ì§€ì› (í™•ì¸ë¨)**:
- Gemma ê³„ì—´ (gemma3:1b/12b/27b, gemma3n, codegemma)
- Phi ê³„ì—´ (phi3, phi4:14b - phi4-minië§Œ ì§€ì›!)
- CodeLlama, Codestral, DeepCoder, OpenCoder
- GLM4, Yi, StableLM2, Vicuna, Solar
- DeepSeek-R1, OLMo, TinyLlama
- íŒŒì¸íŠœë‹ ë³€í˜• (dolphin-*, samantha-*, yarn-*)

**Example workflow**:
```
User: "Count files in current directory"
  â†“
Agent â†’ bash(ls | wc -l)
  â†“
Result: "42"
  â†“
Agent: "There are 42 files in the current directory."
```

## Development

```bash
# Format code
dune fmt

# Run tests
dune test

# Build in watch mode
dune build -w
```

## License

MIT
